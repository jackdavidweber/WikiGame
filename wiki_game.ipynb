{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wikipedia Game\n",
    "The goal of this project is to build an algorithm that is able to perform better than a random player at the wikipedia game! The wikipedia game is [INSERT INFO ABOUT WIKIPEDIA GAME]. \n",
    "\n",
    "We've broken the project into our main technical challenges\n",
    "* Scrape wikipedia (or at least a subset) in order to build a data structure with information about what links exist between pages.\n",
    "* Use graph algorithms to learn cool things about our graph (including the shortest distance between two pages)!\n",
    "* Create an algorithm that does not rely on the graph but can still navigate from one page to another in a short distance. Ideally this distance would be equal to the shortest possible distance, but it would be cool if it at least performed better than an algorithm that chooses links randomly.\n",
    "* Compare algorithm to a human and random competitor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scraping Wikipedia\n",
    "The easiest way to do this would probably have been to rely on the wikipedia data dumps that are released twice monthly. However, we wanted to play with beautiful soup and requests, so we decided to implement the scraping manually. This is much more time intensive (computationally) which means that we are not able to build a graph of all of wikipedia, but we also found it to be way cooler!\n",
    "\n",
    "Before we could actually scrape wikipedia, we had to decide what data we wanted to store and how we wanted to store it. After looking through a bunch of articles, we found that all wikipedia pages have the same link structure:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/[articleTitle]\n",
    "```\n",
    "So the article on Star Wars has the link https://en.wikipedia.org/wiki/Star_Wars and the article entitled \"The Hitchhiker's Guide to the Galaxy\" has the link https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy (notice how the apostraphe is handled!).\n",
    "\n",
    "This meant that as long as we stored the article title, we could always recreate the link for the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the requested page is <title>The Hitchhiker's Guide to the Galaxy - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"lxml\"\n",
    "\n",
    "title_0 = r\"The_Hitchhiker%27s_Guide_to_the_Galaxy\"\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/\" + title_0\n",
    "response = requests.get(wiki_url)\n",
    "soup = BeautifulSoup(response.text,PARSER)\n",
    "\n",
    "print(f\"The title of the requested page is {soup.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is how do we find all of the article names linked to within a given article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get the first 10 <a> html elements\n",
    "a_elements = soup.find_all('a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, '/wiki/File:The_Hitchhikers_Guide_to_the_Galaxy_Part_1.ogg', '#mw-head', '#searchInput', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)', '/wiki/Hitchhiker%27s_Guide_(disambiguation)', '/wiki/File:H2G2_UK_front_cover.jpg', '/wiki/Douglas_Adams', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy:_The_Original_Radio_Scripts']\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the href part of each <a> element\n",
    "links = []\n",
    "for element in a_elements:\n",
    "    links.append(element.get('href'))\n",
    "\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly all of the links that start with \"/wiki/\" are wikipedia pages! So let's filter out everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/File:The_Hitchhikers_Guide_to_the_Galaxy_Part_1.ogg',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)',\n",
       " '/wiki/Hitchhiker%27s_Guide_(disambiguation)',\n",
       " '/wiki/File:H2G2_UK_front_cover.jpg',\n",
       " '/wiki/Douglas_Adams',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy:_The_Original_Radio_Scripts']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_links = []\n",
    "for link in links:\n",
    "    if (link and link[:6] == \"/wiki/\"):\n",
    "        clean_links.append(link)\n",
    "\n",
    "links = clean_links.copy()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably also don't want the Files. Let's get rid of these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)',\n",
       " '/wiki/Hitchhiker%27s_Guide_(disambiguation)',\n",
       " '/wiki/Douglas_Adams',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_links = []\n",
    "for link in links:\n",
    "\n",
    "    # We found that any link with a : in it is not a \"regular\" wikipedia\n",
    "    # Page so we exclude them all\n",
    "    if (link and link.find(\":\") == -1):\n",
    "        clean_links.append(link)\n",
    "\n",
    "links = clean_links.copy()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combined this scraping and filtering into our `get_wiki_graph_one_step` function. It takes a list of wikipedia article titles. It returns a dictionary where each article is a key and the corresponding value is a list of all the articles linked to within that article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/2): The_Hitchhiker%27s_Guide_to_the_Galaxy\n",
      "(1/2): Star_Wars\n",
      "\n",
      "The_Hitchhiker%27s_Guide_to_the_Galaxy links to 298 other wikipedia articles\n",
      "For example, within the The_Hitchhiker%27s_Guide_to_the_Galaxy article, there are links to the following wikipedia pages:\n",
      "\tRobbie_Stamp\n",
      "\tThe_Hitchhiker%27s_Guide_to_the_Galaxy_(TV_series)\n",
      "\tPeter_Jones_(actor)\n",
      "\tLa_Boite_Theatre_Company\n",
      "\tHyperion_(publisher)\n",
      "\tAlan_J._W._Bell\n",
      "\tExtraterrestrial_life_in_popular_culture\n",
      "\tZaphod_Beeblebrox\n",
      "\tMiriam_Margolyes\n",
      "\tSaeed_Jaffrey\n",
      "\n",
      "Star_Wars links to 1258 other wikipedia articles\n",
      "For example, within the Star_Wars article, there are links to the following wikipedia pages:\n",
      "\tObi-Wan_Kenobi_(TV_series)\n",
      "\tMilitary_order_(religious_society)\n",
      "\tLuke_Skywalker_and_the_Shadows_of_Mindor\n",
      "\tBo-Katan_Kryze\n",
      "\tKathleen_Kennedy_(producer)\n",
      "\tThe_Ewok_Adventure\n",
      "\tAction_Man_(1993%E2%80%932006_toyline)\n",
      "\tAvalon_Hill\n",
      "\tList_of_Star_Wars_Rebels_characters\n",
      "\tStar_Wars_Tales\n"
     ]
    }
   ],
   "source": [
    "from scraping import get_wiki_graph_one_step, get_wiki_graph\n",
    "\n",
    "title_1 = \"Star_Wars\"\n",
    "d = get_wiki_graph_one_step([title_0, title_1])\n",
    "\n",
    "print()\n",
    "print(f\"{title_0} links to {len(d[title_0])} other wikipedia articles\")\n",
    "print(f\"For example, within the {title_0} article, there are links to the following wikipedia pages:\")\n",
    "for i in range(0,min(10, len(d[title_0]))):\n",
    "    print(f\"\\t{d[title_0][i]}\")\n",
    "\n",
    "print()\n",
    "print(f\"{title_1} links to {len(d[title_1])} other wikipedia articles\")\n",
    "print(f\"For example, within the {title_1} article, there are links to the following wikipedia pages:\")\n",
    "for i in range(0,min(10, len(d[title_1]))):\n",
    "    print(f\"\\t{d[title_1][i]}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just requested the star wars wikipedia page. But in order to build a bigger graph, we could then repeat the same process for all of the pages referenced in the star wars page. We could continue to repeat this process for as many steps as we'd like.\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): New_York_State_Route_373\n",
      "(0/59): The_New_York_Times\n",
      "(1/59): Numbered_highways_in_New_York\n",
      "(2/59): Lake_Champlain\n",
      "(3/59): United_States\n",
      "(4/59): 52nd_New_York_State_Legislature\n",
      "(5/59): Vermont\n",
      "(6/59): Parkways_in_New_York\n",
      "(7/59): ISBN_(identifier)\n",
      "(8/59): Main_Page\n",
      "(9/59): Theodore_Roosevelt_International_Highway\n",
      "(10/59): Ausable_Chasm\n",
      "(11/59): American_Antiquarian_Society\n",
      "(12/59): Canadian_Pacific_Railway\n",
      "(13/59): New_York_State_Legislature\n",
      "(14/59): Keeseville,_New_York\n",
      "(15/59): Hamlet_(New_York)\n",
      "(16/59): St._Lawrence_County,_New_York\n",
      "(17/59): List_of_reference_routes_in_New_York\n",
      "(18/59): Burlington,_Vermont\n",
      "(19/59): New_York_State_Route_374\n",
      "(20/59): Plattsburgh,_New_York\n",
      "(21/59): General_Drafting\n",
      "(22/59): Ausable_Chasm,_New_York\n",
      "(23/59): U.S._Route_9_in_New_York\n",
      "(24/59): County_Route_17_(Essex_County,_New_York)\n",
      "(25/59): Standard_Oil_Company_of_New_York\n",
      "(26/59): List_of_Interstate_Highways_in_New_York\n",
      "(27/59): Burlington%E2%80%93Port_Kent_Ferry\n",
      "(28/59): Portland,_Oregon\n",
      "(29/59): Portland,_Maine\n",
      "(30/59): Port_Kent_(Amtrak_station)\n",
      "(31/59): Lake_Champlain_Transportation_Company#Burlington-Port_Kent\n",
      "(32/59): Google_Maps\n",
      "(33/59): List_of_U.S._Routes_in_New_York\n",
      "(34/59): List_of_state_routes_in_New_York\n",
      "(35/59): Adirondack_Park\n",
      "(36/59): Lake_Champlain_Transportation_Company\n",
      "(37/59): History_of_the_iron_and_steel_industry_in_the_United_States#Early_republic\n",
      "(38/59): Google\n",
      "(39/59): New_York_State_Route_372\n",
      "(40/59): Chesterfield,_New_York\n",
      "(41/59): Auto_trail\n",
      "(42/59): Toll_road\n",
      "(43/59): New_York_(state)\n",
      "(44/59): 1930_state_highway_renumbering_(New_York)\n",
      "(45/59): Interstate_87_(New_York)\n",
      "(46/59): Amtrak\n",
      "(47/59): Port_Kent_and_Hopkinton_Turnpike\n",
      "(48/59): Essex_County,_New_York\n",
      "(49/59): Port_Kent,_New_York\n",
      "(50/59): County_Route_71_(Essex_County,_New_York)\n",
      "(51/59): Hopkinton,_New_York\n",
      "(52/59): Albany,_New_York\n",
      "(53/59): Toll_gate\n",
      "(54/59): Reference_route_(New_York)\n",
      "(55/59): Ausable_River_(New_York)\n",
      "(56/59): New_York_Constitution#Constitutional_Convention_of_1894\n",
      "(57/59): Baltimore,_Maryland\n",
      "(58/59): New_York_State_Department_of_Transportation\n"
     ]
    }
   ],
   "source": [
    "title = \"New_York_State_Route_373\"\n",
    "out = get_wiki_graph(starting_refs=[title], num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_York_State_Route_373 has 60 links\n",
      "One such link is The_New_York_Times which itself has 927 links all of which link out to other articles\n"
     ]
    }
   ],
   "source": [
    "d = out[0]\n",
    "print(f\"{title} has {len(d[title])} links\")\n",
    "\n",
    "title_1 = d[title][0]\n",
    "print(f\"One such link is {title_1} which itself has {len(d[title_1])} links all of which link out to other articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we did this for two steps. In the first step we found all of the links in `New_York_State_Route_373`. In the second step we found all of the links within all of these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we were to do the third step, we would have to find all of the links within 19530 wikipedia articles\n",
      "Given that this takes about 0.5 seconds per article, just three steps would take 9765.0 seconds or 2.7125 hours\n"
     ]
    }
   ],
   "source": [
    "print(f\"If we were to do the third step, we would have to find all of the links within {len(out[2])} wikipedia articles\")\n",
    "\n",
    "print(f\"Given that this takes about 0.5 seconds per article, just three steps would take {len(out[2])*0.5} seconds or {len(out[2])*0.5/3600} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With every additional step we take beyond the initial article, the time to build the graph increases exponentially. Since we didn't want our personal computers to be occupied for multiple days, we did this on the Pomona servers using the `scraping.py` file we wrote. The final product was a .json file. In the next section, we will analyze this json and use it to construct a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Graph Algorithms\n",
    "Let's first load in and take a look at the json we pulled in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"wiki_graph_NYSR_2.json\")\n",
    "\n",
    "d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our json contains a total of 19604 entries where each entry is a key value pair.\n",
      "The key is the name of a wikipedia page. The value is a list of pages linked to within that wikipedia page.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our json contains a total of {len(d.keys())} entries where each entry is a key value pair.\")\n",
    "print(f\"The key is the name of a wikipedia page. The value is a list of pages linked to within that wikipedia page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the networkx library to construct a directed graph from the json. The graph is directed since page links do not imply reverse page links (i.e. just because Pineapple links to 17th century does not mean that 17th century links to pineaple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g = nx.DiGraph(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networkx library allows us to easily compute shortest paths between wikipedia pages using [Dijkstra's Algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 1 clicks to go from New_York_State_Route_373 to New_York_State_Legislature\n",
      "It takes 2 clicks to go from New_York_State_Route_373 to Enos_T._Throop\n"
     ]
    }
   ],
   "source": [
    "source = \"New_York_State_Route_373\"\n",
    "\n",
    "# How many links do you need to click to get from NYSR_373 to NYS Legislature?\n",
    "target = \"New_York_State_Legislature\"\n",
    "shortest_path = nx.shortest_path(g, source=source, target=target)\n",
    "print(f\"It takes {len(shortest_path)-1} clicks to go from {source} to {target}\")\n",
    "\n",
    "# How many links do you need to click to get from NYSR_373 to \"Enos_T._Throop\"?\n",
    "target = \"Enos_T._Throop\"\n",
    "shortest_path = nx.shortest_path(g, source=source, target=target)\n",
    "print(f\"It takes {len(shortest_path)-1} clicks to go from {source} to {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a way of finding the shortest path between any two wikipedia pages that we scraped. Let's see if we can develop other algorithms for finding the shortest path without access to the graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Other Algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human being playing the Wikipedia game doesn't have access to the entire graph of Wikipedia pages (if they did, then the game would be a lot easier!) So we wanted to create an algorithm that emulates the behaviour of a human player. When we look for the next link to click, we typically assess how similar each link on a page is to the target page we want to get to. We can do this algorithmically by computing the _semantic similarity_ of each link name with the target page. This yields a greedy algorithm where at each step, we choose the link that is most similar to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki_graph.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 22:27:05.712890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-17 22:27:05.712916: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-17 22:27:07.412279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-17 22:27:07.412308: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-17 22:27:07.412328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fedora-ux333): /proc/driver/nvidia/version does not exist\n",
      "2022-04-17 22:27:07.412490: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the language model from Tensorflow\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(target, links):\n",
    "    '''\n",
    "    Given a target, find the link with highest semantic similarity\n",
    "    '''\n",
    "    embeddings = embed([target] + links).numpy()\n",
    "    cosines = np.dot(embeddings[0], embeddings[1:].T)\n",
    "    return [links[i] for i in cosines.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(start, end):\n",
    "    '''\n",
    "    Apply the semantic similarity greedy search algorithm to find a path from start to end.\n",
    "    '''\n",
    "    to_visit = [start]\n",
    "    visited = defaultdict(bool)\n",
    "    previous = defaultdict(str)\n",
    "\n",
    "    while len(to_visit) > 0:\n",
    "        current = to_visit.pop()\n",
    "\n",
    "        visited[current] = True\n",
    "        if current == end:\n",
    "            break\n",
    "        if current not in data:\n",
    "            continue\n",
    "\n",
    "        links = [l for l in data[current] if not visited[l]]\n",
    "        links = predict(end, links)\n",
    "        for l in links:\n",
    "            to_visit.append(l)\n",
    "            previous[l] = current\n",
    "\n",
    "    if visited[end]:\n",
    "        path = [end]\n",
    "        current = end\n",
    "        while previous[current]:\n",
    "            current = previous[current]\n",
    "            path.append(current)\n",
    "        return list(reversed(path))\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373', 'American Antiquarian Society']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'American Antiquarian Society')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373', 'United States', 'U.S. state']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'U.S. state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works for these two examples, so we have a proof of concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373',\n",
       " 'St. Lawrence County, New York',\n",
       " 'Hopkinton, New York',\n",
       " 'Adirondack Park',\n",
       " 'Ausable River (New York)',\n",
       " 'Keeseville, New York',\n",
       " 'Chesterfield, New York',\n",
       " 'Port Kent, New York',\n",
       " 'Essex County, New York',\n",
       " 'Albany, New York',\n",
       " 'Plattsburgh, New York',\n",
       " 'Lake Champlain Transportation Company',\n",
       " 'Lake Champlain',\n",
       " 'New York (state)',\n",
       " 'New York State Legislature',\n",
       " '52nd New York State Legislature',\n",
       " 'United States',\n",
       " 'Amtrak',\n",
       " 'Canadian Pacific Railway',\n",
       " 'Vermont',\n",
       " 'Burlington, Vermont',\n",
       " 'Portland, Maine',\n",
       " 'Theodore Roosevelt International Highway',\n",
       " 'Auto trail',\n",
       " 'Baltimore, Maryland',\n",
       " 'Portland, Oregon',\n",
       " 'American Antiquarian Society',\n",
       " 'Henry Louis Gates']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'Henry Louis Gates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one page links to Henry Louis Gates, which is American Antiquarian Society). Unfortunately American Antiquarian Society is semantically very far from Henry Louis Gates, so it takes a long time for the algorithm to click on American Antiquarian Society, even though it's actually linked to by the starting page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Realization: Try it Live!\n",
    "So far we have been working entirely with a graph that we scraped off wikipedia and stored locally on our machine. This makes the algorithms run quickly since the data are accessible without having to make network requests. However it also means that we are only able to work with a relatively small subgraph of the total wikipedia graph. \n",
    "\n",
    "One solution would have been to just scrape more of the wikipedia graph. But this takes a long time as we discuss above.\n",
    "\n",
    "So instead we decided just to make some minor modifications in order to run our algorithm on the live version of wikipedia!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stephen Hawking\n",
      "Age of the universe\n",
      "Universe\n",
      "Number\n",
      "Multicomplex number\n",
      "P-adic number\n",
      "Computable number\n",
      "Superreal number\n",
      "Super-real number\n",
      "Split-complex number\n",
      "Hypercomplex number\n",
      "Dual-complex number\n",
      "Bicomplex number\n",
      "Hyperreal number\n",
      "Ordinal number\n",
      "0 (number)\n",
      "42 (number)\n"
     ]
    }
   ],
   "source": [
    "from WikipediaSearch import WikipediaSearch\n",
    "\n",
    "source = \"Stephen Hawking\"  # The page the player is starting at\n",
    "target = \"42 (number)\"  # The page where the player is going\n",
    "\n",
    "player = WikipediaSearch(\"semantic\", verbose=True)\n",
    "path = player.search(source, target, limit=25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Player traveled from Stephen Hawking to 42 (number) in 17 clicks\n"
     ]
    }
   ],
   "source": [
    "print(f\"Player traveled from {source} to {target} in {len(path)} clicks!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wohoo! In the snippet above, our semantic player was able to get from Stephen Hawking to 42 by requesting Wikipedia directly. We are not passing in a stored graph or any other data structure with wikipedia information. \n",
    "\n",
    "This allows us to make all sorts of other explorations that we discuss in the next section!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Competition!\n",
    "Let's see how our heuristic does against a human and random player! To gather human data, we used [The Wiki Game](https://www.thewikigame.com/group) which allows humans to compete against each other in the Wikipedia Game. The game runs online every 2 minutes and posts the winner and path for every game. Manually we went through and pulled 30 games into a spreadsheet.\n",
    "\n",
    "Each of the 30 games had a source page and target page. The random and heuristic players played each of the 30 games and their scores were added to the spreadsheet. All of the code and commentary for this can be found in `competition.ipynb`\n",
    "\n",
    "Let's load in the final competition spreadsheet:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>source</th>\n",
       "      <th>target</th>\n",
       "      <th>human_path</th>\n",
       "      <th>semantic_path</th>\n",
       "      <th>random_path</th>\n",
       "      <th>human_path_length</th>\n",
       "      <th>semantic_path_length</th>\n",
       "      <th>random_path_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Romance languages</td>\n",
       "      <td>Great Barrier Reef</td>\n",
       "      <td>['Romance languages', 'Africa', 'Asia', 'Pacif...</td>\n",
       "      <td>['Romance languages', 'Australia', 'Great Barr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Natalie Portman</td>\n",
       "      <td>Proto-Indo-European language</td>\n",
       "      <td>['Natalie Portman', 'United States nationality...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>IBM</td>\n",
       "      <td>Human eye</td>\n",
       "      <td>['IBM', 'Artificial intelligence', 'Facial rec...</td>\n",
       "      <td>['IBM', 'Human resources management', 'Human c...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>24.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Academy Awards</td>\n",
       "      <td>Telecommunications network</td>\n",
       "      <td>['Academy Awards', 'American Broadcasting Comp...</td>\n",
       "      <td>['Academy Awards', 'Streaming service provider...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>Folk music</td>\n",
       "      <td>John Krasinski</td>\n",
       "      <td>['Folk music', 'American folk music', 'Music o...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>Kindergarten</td>\n",
       "      <td>John Calvin</td>\n",
       "      <td>['Kindergarten', 'Education in the United Stat...</td>\n",
       "      <td>['Kindergarten', 'Paul Monroe', 'John Dewey', ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>Glucose</td>\n",
       "      <td>Nintendo Entertainment System</td>\n",
       "      <td>['Glucose', 'WHO Model List of Essential Medic...</td>\n",
       "      <td>['Glucose', 'Reward system', 'Video game addic...</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>Planet</td>\n",
       "      <td>BBC One</td>\n",
       "      <td>['Planet', 'Galileo Galilei', 'Thomas Harriot'...</td>\n",
       "      <td>['Planet', 'BBC News', 'BBC One']</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>Text messaging</td>\n",
       "      <td>Resurrection of Jesus</td>\n",
       "      <td>['Text messaging', 'Santa Claus', 'Easter Bunn...</td>\n",
       "      <td>['Text messaging', 'Esprit de corps', 'Esprit ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Charles Bronson</td>\n",
       "      <td>Role-playing game</td>\n",
       "      <td>['Charles Bronson', 'Vigilante', 'Duel', '1908...</td>\n",
       "      <td>['Charles Bronson', 'Combat!', 'Combat (disamb...</td>\n",
       "      <td>[]</td>\n",
       "      <td>11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>10</td>\n",
       "      <td>Wavelength</td>\n",
       "      <td>Kelly Clarkson</td>\n",
       "      <td>['Wavelength', 'Sound', 'Musical acoustics', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>11</td>\n",
       "      <td>Germanium</td>\n",
       "      <td>Science</td>\n",
       "      <td>['Germanium', 'Chemical element', 'Science']</td>\n",
       "      <td>['Germanium', 'History of chemistry', 'Histori...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>12</td>\n",
       "      <td>The Doors</td>\n",
       "      <td>Nobel Prize in Physiology or Medicine</td>\n",
       "      <td>['The Doors', 'Rock music', 'Popular music', '...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>13</td>\n",
       "      <td>Lead</td>\n",
       "      <td>DNA replication</td>\n",
       "      <td>['Lead', 'Enzyme', 'Protein', 'DNA replication']</td>\n",
       "      <td>['Lead', 'Homology (chemistry)', 'Homology (di...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>14</td>\n",
       "      <td>Jacqueline Kennedy Onassis</td>\n",
       "      <td>Humanism</td>\n",
       "      <td>['Jacqueline Kennedy Onassis', 'Academy of Tel...</td>\n",
       "      <td>['Jacqueline Kennedy Onassis', 'Aristotle Onas...</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>15</td>\n",
       "      <td>Marco Polo</td>\n",
       "      <td>Atmosphere</td>\n",
       "      <td>['Marco Polo', 'Fra Mauro map', 'NASA', 'Atmos...</td>\n",
       "      <td>['Marco Polo', 'Shambhala', 'Soundscape', 'Amb...</td>\n",
       "      <td>[]</td>\n",
       "      <td>10</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>16</td>\n",
       "      <td>Diabetes mellitus</td>\n",
       "      <td>Health care</td>\n",
       "      <td>['Diabetes mellitus', 'Insulin', 'Diabetes mel...</td>\n",
       "      <td>['Diabetes mellitus', 'Health policy', 'Health...</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>17</td>\n",
       "      <td>Astronaut</td>\n",
       "      <td>Family</td>\n",
       "      <td>['Astronaut', 'Kennedy Space Center', 'John F....</td>\n",
       "      <td>['Astronaut', 'Asian people', 'Dougla people',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>23.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>18</td>\n",
       "      <td>Minnesota</td>\n",
       "      <td>Hayley Williams</td>\n",
       "      <td>['Minnesota', 'United States', 'Mississippian ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>19</td>\n",
       "      <td>Sheep</td>\n",
       "      <td>Bruce Lee</td>\n",
       "      <td>['Sheep', 'China', 'Chinese martial arts', 'Br...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>20</td>\n",
       "      <td>Fishing</td>\n",
       "      <td>Drug</td>\n",
       "      <td>['Fishing', 'Kentucky', 'United States', 'Drug...</td>\n",
       "      <td>['Fishing', 'Bodyboarding', 'Food', 'Narcotic'...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>21</td>\n",
       "      <td>International Olympic Committee</td>\n",
       "      <td>Glass</td>\n",
       "      <td>['International Olympic Committee', 'Switzerla...</td>\n",
       "      <td>['International Olympic Committee', 'Quadrenni...</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>22</td>\n",
       "      <td>Zhang Ziyi</td>\n",
       "      <td>Southeast Europe</td>\n",
       "      <td>['Zhang Ziyi', 'China', 'List of countries and...</td>\n",
       "      <td>['Zhang Ziyi', 'South Korea', 'South Chungcheo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>11.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>23</td>\n",
       "      <td>SWAT</td>\n",
       "      <td>New Testament</td>\n",
       "      <td>['SWAT', 'September 11 attacks', 'United State...</td>\n",
       "      <td>['SWAT', 'Christian Parenti', 'New College of ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>5</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>24</td>\n",
       "      <td>Wheat</td>\n",
       "      <td>Doctorate</td>\n",
       "      <td>['Wheat', 'United States', 'Public university'...</td>\n",
       "      <td>['Wheat', 'Bioenergy', 'Arborist', 'Master%27s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>25</td>\n",
       "      <td>Sarah Palin</td>\n",
       "      <td>Ship</td>\n",
       "      <td>['Sarah Palin', 'List of Governors of Alaska',...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>26</td>\n",
       "      <td>Tool</td>\n",
       "      <td>Sunlight</td>\n",
       "      <td>['Tool', 'Glasses', 'Light', 'Sunlight']</td>\n",
       "      <td>['Tool', 'Solar power', 'Sunlight']</td>\n",
       "      <td>[]</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>27</td>\n",
       "      <td>Education</td>\n",
       "      <td>Extraterrestrial life</td>\n",
       "      <td>['Education', 'Natural science', 'Science', 'A...</td>\n",
       "      <td>['Education', 'Epistemology', 'Philosophy of l...</td>\n",
       "      <td>[]</td>\n",
       "      <td>6</td>\n",
       "      <td>9.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>28</td>\n",
       "      <td>Logo</td>\n",
       "      <td>Writing system</td>\n",
       "      <td>['Logo', 'Greek language', 'Writing system']</td>\n",
       "      <td>['Logo', 'Spencerian Script', 'Teaching script...</td>\n",
       "      <td>[]</td>\n",
       "      <td>3</td>\n",
       "      <td>7.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>29</td>\n",
       "      <td>Motion (physics)</td>\n",
       "      <td>Superhero film</td>\n",
       "      <td>['Motion (physics)', 'Force', 'Force (disambig...</td>\n",
       "      <td>['Motion (physics)', 'Fictitious force', 'Edwa...</td>\n",
       "      <td>[]</td>\n",
       "      <td>9</td>\n",
       "      <td>6.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Unnamed: 0                           source  \\\n",
       "0            0                Romance languages   \n",
       "1            1                  Natalie Portman   \n",
       "2            2                              IBM   \n",
       "3            3                   Academy Awards   \n",
       "4            4                       Folk music   \n",
       "5            5                     Kindergarten   \n",
       "6            6                          Glucose   \n",
       "7            7                           Planet   \n",
       "8            8                   Text messaging   \n",
       "9            9                  Charles Bronson   \n",
       "10          10                       Wavelength   \n",
       "11          11                        Germanium   \n",
       "12          12                        The Doors   \n",
       "13          13                             Lead   \n",
       "14          14       Jacqueline Kennedy Onassis   \n",
       "15          15                       Marco Polo   \n",
       "16          16                Diabetes mellitus   \n",
       "17          17                        Astronaut   \n",
       "18          18                        Minnesota   \n",
       "19          19                            Sheep   \n",
       "20          20                          Fishing   \n",
       "21          21  International Olympic Committee   \n",
       "22          22                       Zhang Ziyi   \n",
       "23          23                             SWAT   \n",
       "24          24                            Wheat   \n",
       "25          25                      Sarah Palin   \n",
       "26          26                             Tool   \n",
       "27          27                        Education   \n",
       "28          28                             Logo   \n",
       "29          29                 Motion (physics)   \n",
       "\n",
       "                                   target  \\\n",
       "0                      Great Barrier Reef   \n",
       "1            Proto-Indo-European language   \n",
       "2                               Human eye   \n",
       "3              Telecommunications network   \n",
       "4                          John Krasinski   \n",
       "5                             John Calvin   \n",
       "6           Nintendo Entertainment System   \n",
       "7                                 BBC One   \n",
       "8                   Resurrection of Jesus   \n",
       "9                       Role-playing game   \n",
       "10                         Kelly Clarkson   \n",
       "11                                Science   \n",
       "12  Nobel Prize in Physiology or Medicine   \n",
       "13                        DNA replication   \n",
       "14                               Humanism   \n",
       "15                             Atmosphere   \n",
       "16                            Health care   \n",
       "17                                 Family   \n",
       "18                        Hayley Williams   \n",
       "19                              Bruce Lee   \n",
       "20                                   Drug   \n",
       "21                                  Glass   \n",
       "22                       Southeast Europe   \n",
       "23                          New Testament   \n",
       "24                              Doctorate   \n",
       "25                                   Ship   \n",
       "26                               Sunlight   \n",
       "27                  Extraterrestrial life   \n",
       "28                         Writing system   \n",
       "29                         Superhero film   \n",
       "\n",
       "                                           human_path  \\\n",
       "0   ['Romance languages', 'Africa', 'Asia', 'Pacif...   \n",
       "1   ['Natalie Portman', 'United States nationality...   \n",
       "2   ['IBM', 'Artificial intelligence', 'Facial rec...   \n",
       "3   ['Academy Awards', 'American Broadcasting Comp...   \n",
       "4   ['Folk music', 'American folk music', 'Music o...   \n",
       "5   ['Kindergarten', 'Education in the United Stat...   \n",
       "6   ['Glucose', 'WHO Model List of Essential Medic...   \n",
       "7   ['Planet', 'Galileo Galilei', 'Thomas Harriot'...   \n",
       "8   ['Text messaging', 'Santa Claus', 'Easter Bunn...   \n",
       "9   ['Charles Bronson', 'Vigilante', 'Duel', '1908...   \n",
       "10  ['Wavelength', 'Sound', 'Musical acoustics', '...   \n",
       "11       ['Germanium', 'Chemical element', 'Science']   \n",
       "12  ['The Doors', 'Rock music', 'Popular music', '...   \n",
       "13   ['Lead', 'Enzyme', 'Protein', 'DNA replication']   \n",
       "14  ['Jacqueline Kennedy Onassis', 'Academy of Tel...   \n",
       "15  ['Marco Polo', 'Fra Mauro map', 'NASA', 'Atmos...   \n",
       "16  ['Diabetes mellitus', 'Insulin', 'Diabetes mel...   \n",
       "17  ['Astronaut', 'Kennedy Space Center', 'John F....   \n",
       "18  ['Minnesota', 'United States', 'Mississippian ...   \n",
       "19  ['Sheep', 'China', 'Chinese martial arts', 'Br...   \n",
       "20  ['Fishing', 'Kentucky', 'United States', 'Drug...   \n",
       "21  ['International Olympic Committee', 'Switzerla...   \n",
       "22  ['Zhang Ziyi', 'China', 'List of countries and...   \n",
       "23  ['SWAT', 'September 11 attacks', 'United State...   \n",
       "24  ['Wheat', 'United States', 'Public university'...   \n",
       "25  ['Sarah Palin', 'List of Governors of Alaska',...   \n",
       "26           ['Tool', 'Glasses', 'Light', 'Sunlight']   \n",
       "27  ['Education', 'Natural science', 'Science', 'A...   \n",
       "28       ['Logo', 'Greek language', 'Writing system']   \n",
       "29  ['Motion (physics)', 'Force', 'Force (disambig...   \n",
       "\n",
       "                                        semantic_path random_path  \\\n",
       "0   ['Romance languages', 'Australia', 'Great Barr...          []   \n",
       "1                                                  []          []   \n",
       "2   ['IBM', 'Human resources management', 'Human c...          []   \n",
       "3   ['Academy Awards', 'Streaming service provider...          []   \n",
       "4                                                  []          []   \n",
       "5   ['Kindergarten', 'Paul Monroe', 'John Dewey', ...          []   \n",
       "6   ['Glucose', 'Reward system', 'Video game addic...          []   \n",
       "7                   ['Planet', 'BBC News', 'BBC One']          []   \n",
       "8   ['Text messaging', 'Esprit de corps', 'Esprit ...          []   \n",
       "9   ['Charles Bronson', 'Combat!', 'Combat (disamb...          []   \n",
       "10                                                 []          []   \n",
       "11  ['Germanium', 'History of chemistry', 'Histori...          []   \n",
       "12                                                 []          []   \n",
       "13  ['Lead', 'Homology (chemistry)', 'Homology (di...          []   \n",
       "14  ['Jacqueline Kennedy Onassis', 'Aristotle Onas...          []   \n",
       "15  ['Marco Polo', 'Shambhala', 'Soundscape', 'Amb...          []   \n",
       "16  ['Diabetes mellitus', 'Health policy', 'Health...          []   \n",
       "17  ['Astronaut', 'Asian people', 'Dougla people',...          []   \n",
       "18                                                 []          []   \n",
       "19                                                 []          []   \n",
       "20  ['Fishing', 'Bodyboarding', 'Food', 'Narcotic'...          []   \n",
       "21  ['International Olympic Committee', 'Quadrenni...          []   \n",
       "22  ['Zhang Ziyi', 'South Korea', 'South Chungcheo...          []   \n",
       "23  ['SWAT', 'Christian Parenti', 'New College of ...          []   \n",
       "24  ['Wheat', 'Bioenergy', 'Arborist', 'Master%27s...          []   \n",
       "25                                                 []          []   \n",
       "26                ['Tool', 'Solar power', 'Sunlight']          []   \n",
       "27  ['Education', 'Epistemology', 'Philosophy of l...          []   \n",
       "28  ['Logo', 'Spencerian Script', 'Teaching script...          []   \n",
       "29  ['Motion (physics)', 'Fictitious force', 'Edwa...          []   \n",
       "\n",
       "    human_path_length  semantic_path_length  random_path_length  \n",
       "0                   6                   3.0                 NaN  \n",
       "1                   7                   NaN                 NaN  \n",
       "2                   4                  24.0                 NaN  \n",
       "3                   4                  12.0                 NaN  \n",
       "4                   8                   NaN                 NaN  \n",
       "5                   5                   4.0                 NaN  \n",
       "6                   7                   5.0                 NaN  \n",
       "7                   7                   3.0                 NaN  \n",
       "8                   5                   8.0                 NaN  \n",
       "9                  11                   8.0                 NaN  \n",
       "10                  6                   NaN                 NaN  \n",
       "11                  3                   6.0                 NaN  \n",
       "12                  8                   NaN                 NaN  \n",
       "13                  4                   7.0                 NaN  \n",
       "14                  9                   5.0                 NaN  \n",
       "15                 10                  11.0                 NaN  \n",
       "16                  7                   3.0                 NaN  \n",
       "17                  6                  23.0                 NaN  \n",
       "18                  7                   NaN                 NaN  \n",
       "19                  4                   NaN                 NaN  \n",
       "20                  5                  11.0                 NaN  \n",
       "21                  7                  10.0                 NaN  \n",
       "22                  5                  11.0                 NaN  \n",
       "23                  5                   6.0                 NaN  \n",
       "24                  4                   5.0                 NaN  \n",
       "25                  7                   NaN                 NaN  \n",
       "26                  4                   3.0                 NaN  \n",
       "27                  6                   9.0                 NaN  \n",
       "28                  3                   7.0                 NaN  \n",
       "29                  9                   6.0                 NaN  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"competition_output.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At a high level, let's see how each player did!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_path_length       6.10000\n",
       "semantic_path_length    8.26087\n",
       "random_path_length          NaN\n",
       "dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflens = df[[\"human_path_length\", \"semantic_path_length\", \"random_path_length\",]]\n",
    "dflens.mean(skipna=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random player never succeeds at finding the correct path. This is because we did not let it click more than 25 links without calling it a failure.\n",
    "\n",
    "At first glance the human and semantic player perform rather similarly. However this is not the full picture since the average ignores when the semantic player is unable to get from source to target in 25 clicks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "human_path_length       0.000000\n",
       "semantic_path_length    0.233333\n",
       "random_path_length      1.000000\n",
       "dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dflens.isna().mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The human never fails at reaching the target. The random player always fails. And the semantic player fails 23.3% of the time. \n",
    "\n",
    "Note that this is a bit unfair since our human player is actually many human players. We simply selected the winner of each round and used that to represent the human player. Perhaps if we considered the average human player, the semantic player would perform relatively better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Direction\n",
    "Despite the semester being over, here are some places that we imagine this project could go if we were to continue it:\n",
    "* Semantic player with more information. Perhaps the semantic player is given the first paragraph of the article and draws comparisons using this.\n",
    "* Reinforcement Learning Player. Use the true wikipedia shortest path. Any time that the player moves away from the target, punish. Any time the player moves torwards the target, reward. This raises the question of whether the player would simply learn to memorize the graph. What other information would we need to provide?\n",
    "\n",
    "[Draft presentation](https://docs.google.com/presentation/d/1GxHRYCPbECe-QAT_uPcoYR31yHpl1f1XA6l_k-nwTvw/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c816ed2e4fb958a73b47f4f7bcb3cccc66deb384cf692b066fe3b8c8b16294f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
