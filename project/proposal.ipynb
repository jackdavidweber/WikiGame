{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup     \n",
    "import requests\n",
    "\n",
    "PARSER = \"lxml\"            # to use lxml (the most common), you'll need to install with .../pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars_url = \"https://en.wikipedia.org/wiki/Star_Wars\"\n",
    "response = requests.get(star_wars_url)\n",
    "data_from_url = response.text\n",
    "soup = BeautifulSoup(data_from_url,PARSER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [\"Star Wars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_graph_one_step(l):\n",
    "    \"\"\"\n",
    "    Takes a list of wikipedia articles with no repeats. \n",
    "    Returns A dictionary where keys are items in original list. \n",
    "    Values are lists of references to other wikipedia articles\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    # s = set()\n",
    "\n",
    "    for i in range(len(l)):\n",
    "        title = l[i]\n",
    "        wiki_url = \"https://en.wikipedia.org/wiki/\" + title\n",
    "\n",
    "        # Request Wikipedia and Parse\n",
    "        response = requests.get(wiki_url)\n",
    "        data_from_url = response.text\n",
    "        soup = BeautifulSoup(data_from_url,PARSER)\n",
    "\n",
    "        print(f\"({i}/{len(l)}): {title}\")\n",
    "\n",
    "        # Capture all referenced articles within article\n",
    "        link_set = set()  # Use a set to ensure no repeats\n",
    "        for link in soup.find_all('a'):\n",
    "            s = link.get('href')\n",
    "            if (s and s[:6] == \"/wiki/\"):\n",
    "                ref = s[6:]\n",
    "\n",
    "                # Make sure that title does not include \":\" (means it is not normal wikipedia page)\n",
    "                if (ref.find(\":\") == -1):\n",
    "                    link_set.add(ref)\n",
    "        d[title] = list(link_set)\n",
    "\n",
    "    return d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_lists_to_set(lol):\n",
    "    s = set()\n",
    "    for l in lol:\n",
    "        for item in l:\n",
    "            s.add(item)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_graph(final_d = {}, starting_refs=[], num_steps=1):\n",
    "    if starting_refs:\n",
    "        ref_list = starting_refs\n",
    "    else:\n",
    "        ref_list = [\"New_York_State_Route_373\"]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        step_d = get_wiki_graph_one_step(ref_list)\n",
    "        final_d.update(step_d)\n",
    "\n",
    "        # Get all of the references that haven't been added to graph\n",
    "        step_refs = list_of_lists_to_set(list(step_d.values()))\n",
    "        existing_refs = set(final_d.keys())\n",
    "        unseen_refs = step_refs.difference(existing_refs)\n",
    "\n",
    "        ref_list = list(unseen_refs)\n",
    "\n",
    "    return (final_d, step_refs, ref_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): New_York_State_Route_373\n",
      "(0/59): Auto_trail\n",
      "(1/59): Parkways_in_New_York\n",
      "(2/59): Chesterfield,_New_York\n",
      "(3/59): Port_Kent_and_Hopkinton_Turnpike\n",
      "(4/59): Burlington,_Vermont\n",
      "(5/59): Plattsburgh,_New_York\n",
      "(6/59): ISBN_(identifier)\n",
      "(7/59): County_Route_17_(Essex_County,_New_York)\n",
      "(8/59): 52nd_New_York_State_Legislature\n",
      "(9/59): Interstate_87_(New_York)\n",
      "(10/59): Lake_Champlain\n",
      "(11/59): New_York_State_Legislature\n",
      "(12/59): Theodore_Roosevelt_International_Highway\n",
      "(13/59): Baltimore,_Maryland\n",
      "(14/59): Hopkinton,_New_York\n",
      "(15/59): New_York_State_Route_372\n",
      "(16/59): Lake_Champlain_Transportation_Company#Burlington-Port_Kent\n",
      "(17/59): General_Drafting\n",
      "(18/59): Lake_Champlain_Transportation_Company\n",
      "(19/59): Port_Kent_(Amtrak_station)\n",
      "(20/59): Albany,_New_York\n",
      "(21/59): New_York_State_Route_374\n",
      "(22/59): Ausable_Chasm,_New_York\n",
      "(23/59): Amtrak\n",
      "(24/59): Toll_gate\n",
      "(25/59): Hamlet_(New_York)\n",
      "(26/59): Google_Maps\n",
      "(27/59): List_of_U.S._Routes_in_New_York\n",
      "(28/59): Ausable_Chasm\n",
      "(29/59): Toll_road\n",
      "(30/59): List_of_state_routes_in_New_York\n",
      "(31/59): Portland,_Maine\n",
      "(32/59): 1930_state_highway_renumbering_(New_York)\n",
      "(33/59): United_States\n",
      "(34/59): Standard_Oil_Company_of_New_York\n",
      "(35/59): Main_Page\n",
      "(36/59): St._Lawrence_County,_New_York\n",
      "(37/59): Canadian_Pacific_Railway\n",
      "(38/59): Vermont\n",
      "(39/59): Port_Kent,_New_York\n",
      "(40/59): Adirondack_Park\n",
      "(41/59): U.S._Route_9_in_New_York\n",
      "(42/59): Burlington%E2%80%93Port_Kent_Ferry\n",
      "(43/59): New_York_(state)\n",
      "(44/59): Portland,_Oregon\n",
      "(45/59): List_of_Interstate_Highways_in_New_York\n",
      "(46/59): Reference_route_(New_York)\n",
      "(47/59): New_York_State_Department_of_Transportation\n",
      "(48/59): The_New_York_Times\n",
      "(49/59): Ausable_River_(New_York)\n",
      "(50/59): New_York_Constitution#Constitutional_Convention_of_1894\n",
      "(51/59): County_Route_71_(Essex_County,_New_York)\n",
      "(52/59): History_of_the_iron_and_steel_industry_in_the_United_States#Early_republic\n",
      "(53/59): Keeseville,_New_York\n",
      "(54/59): Google\n",
      "(55/59): Essex_County,_New_York\n",
      "(56/59): List_of_reference_routes_in_New_York\n",
      "(57/59): American_Antiquarian_Society\n",
      "(58/59): Numbered_highways_in_New_York\n"
     ]
    }
   ],
   "source": [
    "final_d, step_refs, ref_list = get_wiki_graph(starting_refs=[\"New_York_State_Route_373\"], num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create json object from dictionary\n",
    "json = json.dumps(final_d)\n",
    "\n",
    "# open file for writing, \"w\" \n",
    "f = open(\"wiki_graph.json\",\"w\")\n",
    "\n",
    "# write json object to file\n",
    "f.write(json)\n",
    "\n",
    "# close file\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nd = {}\\nsteps_remaining = 2\\n\\nwhile steps_remaining > 0 and len(queue)>0:\\n    title = queue.pop(0)\\n    wiki_url = \"https://en.wikipedia.org/wiki/\" + title\\n\\n    # Request Wikipedia and Parse\\n    response = requests.get(wiki_url)\\n    data_from_url = response.text\\n    soup = BeautifulSoup(data_from_url,PARSER)\\n\\n    # Capture all referenced articles within article\\n    link_set = set()  # Use a set to ensure no repeats\\n    for link in soup.find_all(\\'a\\'):\\n        s = link.get(\\'href\\')\\n        if (s and s[:6] == \"/wiki/\"):\\n            ref = s[6:]\\n            link_set.add(ref)\\n            if (title not in d):\\n                queue.append(ref)\\n\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "d = {}\n",
    "steps_remaining = 2\n",
    "\n",
    "while steps_remaining > 0 and len(queue)>0:\n",
    "    title = queue.pop(0)\n",
    "    wiki_url = \"https://en.wikipedia.org/wiki/\" + title\n",
    "\n",
    "    # Request Wikipedia and Parse\n",
    "    response = requests.get(wiki_url)\n",
    "    data_from_url = response.text\n",
    "    soup = BeautifulSoup(data_from_url,PARSER)\n",
    "\n",
    "    # Capture all referenced articles within article\n",
    "    link_set = set()  # Use a set to ensure no repeats\n",
    "    for link in soup.find_all('a'):\n",
    "        s = link.get('href')\n",
    "        if (s and s[:6] == \"/wiki/\"):\n",
    "            ref = s[6:]\n",
    "            link_set.add(ref)\n",
    "            if (title not in d):\n",
    "                queue.append(ref)\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c816ed2e4fb958a73b47f4f7bcb3cccc66deb384cf692b066fe3b8c8b16294f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pyDs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
