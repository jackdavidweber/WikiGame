{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wikipedia Game\n",
    "The goal of this project is to build an algorithm that is able to perform better than a random player at the wikipedia game! The wikipedia game is [INSERT INFO ABOUT WIKIPEDIA GAME]. \n",
    "\n",
    "We've broken the project into three main technical challenges\n",
    "* Scrape wikipedia (or at least a subset) in order to build a data structure with information about what links exist between pages.\n",
    "* Use graph algorithms to learn cool things about our graph (including the shortest distance between two pages)!\n",
    "* Create an algorithm that does not rely on the graph but can still navigate from one page to another in a short distance. Ideally this distance would be equal to the shortest possible distance, but it would be cool if it at least performed better than an algorithm that chooses links randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scraping Wikipedia\n",
    "The easiest way to do this would probably have been to rely on the wikipedia data dumps that are released twice monthly. However, we wanted to play with beautiful soup and requests, so we decided to implement the scraping manually. This is much more time intensive (computationally) which means that we are not able to build a graph of all of wikipedia, but we also found it to be way cooler!\n",
    "\n",
    "Before we could actually scrape wikipedia, we had to decide what data we wanted to store and how we wanted to store it. After looking through a bunch of articles, we found that all wikipedia pages have the same link structure:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/[articleTitle]\n",
    "```\n",
    "So the article on Star Wars has the link https://en.wikipedia.org/wiki/Star_Wars and the article entitled \"The Hitchhiker's Guide to the Galaxy\" has the link https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy (notice how the apostraphe is handled!).\n",
    "\n",
    "This meant that as long as we stored the article title, we could always recreate the link for the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the requested page is <title>The Hitchhiker's Guide to the Galaxy - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"lxml\"\n",
    "\n",
    "title_0 = r\"The_Hitchhiker%27s_Guide_to_the_Galaxy\"\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/\" + title_0\n",
    "response = requests.get(wiki_url)\n",
    "soup = BeautifulSoup(response.text,PARSER)\n",
    "\n",
    "print(f\"The title of the requested page is {soup.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the question is how do we find all of the article names linked to within a given article?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's get the first 10 <a> html elements\n",
    "a_elements = soup.find_all('a')[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, '/wiki/File:The_Hitchhikers_Guide_to_the_Galaxy_Part_1.ogg', '#mw-head', '#searchInput', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)', '/wiki/Hitchhiker%27s_Guide_(disambiguation)', '/wiki/File:H2G2_UK_front_cover.jpg', '/wiki/Douglas_Adams', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases', '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy:_The_Original_Radio_Scripts']\n"
     ]
    }
   ],
   "source": [
    "# Let's extract the href part of each <a> element\n",
    "links = []\n",
    "for element in a_elements:\n",
    "    links.append(element.get('href'))\n",
    "\n",
    "print(links)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly all of the links that start with \"/wiki/\" are wikipedia pages! So let's filter out everything else"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/File:The_Hitchhikers_Guide_to_the_Galaxy_Part_1.ogg',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)',\n",
       " '/wiki/Hitchhiker%27s_Guide_(disambiguation)',\n",
       " '/wiki/File:H2G2_UK_front_cover.jpg',\n",
       " '/wiki/Douglas_Adams',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy:_The_Original_Radio_Scripts']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_links = []\n",
    "for link in links:\n",
    "    if (link and link[:6] == \"/wiki/\"):\n",
    "        clean_links.append(link)\n",
    "\n",
    "links = clean_links.copy()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We probably also don't want the Files. Let's get rid of these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_(disambiguation)',\n",
       " '/wiki/Hitchhiker%27s_Guide_(disambiguation)',\n",
       " '/wiki/Douglas_Adams',\n",
       " '/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy_Primary_and_Secondary_Phases']"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_links = []\n",
    "for link in links:\n",
    "\n",
    "    # We found that any link with a : in it is not a \"regular\" wikipedia\n",
    "    # Page so we exclude them all\n",
    "    if (link and link.find(\":\") == -1):\n",
    "        clean_links.append(link)\n",
    "\n",
    "links = clean_links.copy()\n",
    "links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We combined this scraping and filtering into our `get_wiki_graph_one_step` function. It takes a list of wikipedia article titles. It returns a dictionary where each article is a key and the corresponding value is a list of all the articles linked to within that article."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/2): The_Hitchhiker%27s_Guide_to_the_Galaxy\n",
      "(1/2): Star_Wars\n",
      "\n",
      "The_Hitchhiker%27s_Guide_to_the_Galaxy links to 298 other wikipedia articles\n",
      "For example, within the The_Hitchhiker%27s_Guide_to_the_Galaxy article, there are links to the following wikipedia pages:\n",
      "\tRobbie_Stamp\n",
      "\tThe_Hitchhiker%27s_Guide_to_the_Galaxy_(TV_series)\n",
      "\tPeter_Jones_(actor)\n",
      "\tLa_Boite_Theatre_Company\n",
      "\tHyperion_(publisher)\n",
      "\tAlan_J._W._Bell\n",
      "\tExtraterrestrial_life_in_popular_culture\n",
      "\tZaphod_Beeblebrox\n",
      "\tMiriam_Margolyes\n",
      "\tSaeed_Jaffrey\n",
      "\n",
      "Star_Wars links to 1258 other wikipedia articles\n",
      "For example, within the Star_Wars article, there are links to the following wikipedia pages:\n",
      "\tObi-Wan_Kenobi_(TV_series)\n",
      "\tMilitary_order_(religious_society)\n",
      "\tLuke_Skywalker_and_the_Shadows_of_Mindor\n",
      "\tBo-Katan_Kryze\n",
      "\tKathleen_Kennedy_(producer)\n",
      "\tThe_Ewok_Adventure\n",
      "\tAction_Man_(1993%E2%80%932006_toyline)\n",
      "\tAvalon_Hill\n",
      "\tList_of_Star_Wars_Rebels_characters\n",
      "\tStar_Wars_Tales\n"
     ]
    }
   ],
   "source": [
    "from scraping import get_wiki_graph_one_step, get_wiki_graph\n",
    "\n",
    "title_1 = \"Star_Wars\"\n",
    "d = get_wiki_graph_one_step([title_0, title_1])\n",
    "\n",
    "print()\n",
    "print(f\"{title_0} links to {len(d[title_0])} other wikipedia articles\")\n",
    "print(f\"For example, within the {title_0} article, there are links to the following wikipedia pages:\")\n",
    "for i in range(0,min(10, len(d[title_0]))):\n",
    "    print(f\"\\t{d[title_0][i]}\")\n",
    "\n",
    "print()\n",
    "print(f\"{title_1} links to {len(d[title_1])} other wikipedia articles\")\n",
    "print(f\"For example, within the {title_1} article, there are links to the following wikipedia pages:\")\n",
    "for i in range(0,min(10, len(d[title_1]))):\n",
    "    print(f\"\\t{d[title_1][i]}\")    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just requested the star wars wikipedia page. But in order to build a bigger graph, we could then repeat the same process for all of the pages referenced in the star wars page. We could continue to repeat this process for as many steps as we'd like.\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): New_York_State_Route_373\n",
      "(0/59): The_New_York_Times\n",
      "(1/59): Numbered_highways_in_New_York\n",
      "(2/59): Lake_Champlain\n",
      "(3/59): United_States\n",
      "(4/59): 52nd_New_York_State_Legislature\n",
      "(5/59): Vermont\n",
      "(6/59): Parkways_in_New_York\n",
      "(7/59): ISBN_(identifier)\n",
      "(8/59): Main_Page\n",
      "(9/59): Theodore_Roosevelt_International_Highway\n",
      "(10/59): Ausable_Chasm\n",
      "(11/59): American_Antiquarian_Society\n",
      "(12/59): Canadian_Pacific_Railway\n",
      "(13/59): New_York_State_Legislature\n",
      "(14/59): Keeseville,_New_York\n",
      "(15/59): Hamlet_(New_York)\n",
      "(16/59): St._Lawrence_County,_New_York\n",
      "(17/59): List_of_reference_routes_in_New_York\n",
      "(18/59): Burlington,_Vermont\n",
      "(19/59): New_York_State_Route_374\n",
      "(20/59): Plattsburgh,_New_York\n",
      "(21/59): General_Drafting\n",
      "(22/59): Ausable_Chasm,_New_York\n",
      "(23/59): U.S._Route_9_in_New_York\n",
      "(24/59): County_Route_17_(Essex_County,_New_York)\n",
      "(25/59): Standard_Oil_Company_of_New_York\n",
      "(26/59): List_of_Interstate_Highways_in_New_York\n",
      "(27/59): Burlington%E2%80%93Port_Kent_Ferry\n",
      "(28/59): Portland,_Oregon\n",
      "(29/59): Portland,_Maine\n",
      "(30/59): Port_Kent_(Amtrak_station)\n",
      "(31/59): Lake_Champlain_Transportation_Company#Burlington-Port_Kent\n",
      "(32/59): Google_Maps\n",
      "(33/59): List_of_U.S._Routes_in_New_York\n",
      "(34/59): List_of_state_routes_in_New_York\n",
      "(35/59): Adirondack_Park\n",
      "(36/59): Lake_Champlain_Transportation_Company\n",
      "(37/59): History_of_the_iron_and_steel_industry_in_the_United_States#Early_republic\n",
      "(38/59): Google\n",
      "(39/59): New_York_State_Route_372\n",
      "(40/59): Chesterfield,_New_York\n",
      "(41/59): Auto_trail\n",
      "(42/59): Toll_road\n",
      "(43/59): New_York_(state)\n",
      "(44/59): 1930_state_highway_renumbering_(New_York)\n",
      "(45/59): Interstate_87_(New_York)\n",
      "(46/59): Amtrak\n",
      "(47/59): Port_Kent_and_Hopkinton_Turnpike\n",
      "(48/59): Essex_County,_New_York\n",
      "(49/59): Port_Kent,_New_York\n",
      "(50/59): County_Route_71_(Essex_County,_New_York)\n",
      "(51/59): Hopkinton,_New_York\n",
      "(52/59): Albany,_New_York\n",
      "(53/59): Toll_gate\n",
      "(54/59): Reference_route_(New_York)\n",
      "(55/59): Ausable_River_(New_York)\n",
      "(56/59): New_York_Constitution#Constitutional_Convention_of_1894\n",
      "(57/59): Baltimore,_Maryland\n",
      "(58/59): New_York_State_Department_of_Transportation\n"
     ]
    }
   ],
   "source": [
    "title = \"New_York_State_Route_373\"\n",
    "out = get_wiki_graph(starting_refs=[title], num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_York_State_Route_373 has 60 links\n",
      "One such link is The_New_York_Times which itself has 927 links all of which link out to other articles\n"
     ]
    }
   ],
   "source": [
    "d = out[0]\n",
    "print(f\"{title} has {len(d[title])} links\")\n",
    "\n",
    "title_1 = d[title][0]\n",
    "print(f\"One such link is {title_1} which itself has {len(d[title_1])} links all of which link out to other articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we did this for two steps. In the first step we found all of the links in `New_York_State_Route_373`. In the second step we found all of the links within all of these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we were to do the third step, we would have to find all of the links within 19530 wikipedia articles\n",
      "Given that this takes about 0.5 seconds per article, just three steps would take 9765.0 seconds or 2.7125 hours\n"
     ]
    }
   ],
   "source": [
    "print(f\"If we were to do the third step, we would have to find all of the links within {len(out[2])} wikipedia articles\")\n",
    "\n",
    "print(f\"Given that this takes about 0.5 seconds per article, just three steps would take {len(out[2])*0.5} seconds or {len(out[2])*0.5/3600} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With every additional step we take beyond the initial article, the time to build the graph increases exponentially. Since we didn't want our personal computers to be occupied for multiple days, we did this on the Pomona servers using the `scraping.py` file we wrote. The final product was a .json file. In the next section, we will analyze this json and use it to construct a graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Graph Algorithms\n",
    "Let's first load in and take a look at the json we pulled in step 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "f = open(\"wiki_graph_NYSR_2.json\")\n",
    "\n",
    "d = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our json contains a total of 19604 entries where each entry is a key value pair.\n",
      "The key is the name of a wikipedia page. The value is a list of pages linked to within that wikipedia page.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Our json contains a total of {len(d.keys())} entries where each entry is a key value pair.\")\n",
    "print(f\"The key is the name of a wikipedia page. The value is a list of pages linked to within that wikipedia page.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the networkx library to construct a directed graph from the json. The graph is directed since page links do not imply reverse page links (i.e. just because Pineapple links to 17th century does not mean that 17th century links to pineaple)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "g = nx.DiGraph(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The networkx library allows us to easily compute shortest paths between wikipedia pages using [Dijkstra's Algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It takes 1 clicks to go from New_York_State_Route_373 to New_York_State_Legislature\n",
      "It takes 2 clicks to go from New_York_State_Route_373 to Enos_T._Throop\n"
     ]
    }
   ],
   "source": [
    "source = \"New_York_State_Route_373\"\n",
    "\n",
    "# How many links do you need to click to get from NYSR_373 to NYS Legislature?\n",
    "target = \"New_York_State_Legislature\"\n",
    "shortest_path = nx.shortest_path(g, source=source, target=target)\n",
    "print(f\"It takes {len(shortest_path)-1} clicks to go from {source} to {target}\")\n",
    "\n",
    "# How many links do you need to click to get from NYSR_373 to \"Enos_T._Throop\"?\n",
    "target = \"Enos_T._Throop\"\n",
    "shortest_path = nx.shortest_path(g, source=source, target=target)\n",
    "print(f\"It takes {len(shortest_path)-1} clicks to go from {source} to {target}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! Now we have a way of finding the shortest path between any two wikipedia pages that we scraped. Let's see if we can develop other algorithms for finding the shortest path without access to the graph!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Other Algorithms!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A human being playing the Wikipedia game doesn't have access to the entire graph of Wikipedia pages (if they did, then the game would be a lot easier!) So we wanted to create an algorithm that emulates the behaviour of a human player. When we look for the next link to click, we typically assess how similar each link on a page is to the target page we want to get to. We can do this algorithmically by computing the _semantic similarity_ of each link name with the target page. This yields a greedy algorithm where at each step, we choose the link that is most similar to the target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the graph\n",
    "\n",
    "from collections import defaultdict\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "with open('wiki_graph.json') as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-04-17 22:27:05.712890: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-04-17 22:27:05.712916: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2022-04-17 22:27:07.412279: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-04-17 22:27:07.412308: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-04-17 22:27:07.412328: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (fedora-ux333): /proc/driver/nvidia/version does not exist\n",
      "2022-04-17 22:27:07.412490: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "# Load the language model from Tensorflow\n",
    "# https://www.tensorflow.org/hub/tutorials/semantic_similarity_with_tf_hub_universal_encoder\n",
    "\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "embed = hub.load('https://tfhub.dev/google/universal-sentence-encoder/4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(target, links):\n",
    "    '''\n",
    "    Given a target, find the link with highest semantic similarity\n",
    "    '''\n",
    "    embeddings = embed([target] + links).numpy()\n",
    "    cosines = np.dot(embeddings[0], embeddings[1:].T)\n",
    "    return [links[i] for i in cosines.argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(start, end):\n",
    "    '''\n",
    "    Apply the semantic similarity greedy search algorithm to find a path from start to end.\n",
    "    '''\n",
    "    to_visit = [start]\n",
    "    visited = defaultdict(bool)\n",
    "    previous = defaultdict(str)\n",
    "\n",
    "    while len(to_visit) > 0:\n",
    "        current = to_visit.pop()\n",
    "\n",
    "        visited[current] = True\n",
    "        if current == end:\n",
    "            break\n",
    "        if current not in data:\n",
    "            continue\n",
    "\n",
    "        links = [l for l in data[current] if not visited[l]]\n",
    "        links = predict(end, links)\n",
    "        for l in links:\n",
    "            to_visit.append(l)\n",
    "            previous[l] = current\n",
    "\n",
    "    if visited[end]:\n",
    "        path = [end]\n",
    "        current = end\n",
    "        while previous[current]:\n",
    "            current = previous[current]\n",
    "            path.append(current)\n",
    "        return list(reversed(path))\n",
    "    else:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373', 'American Antiquarian Society']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'American Antiquarian Society')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373', 'United States', 'U.S. state']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'U.S. state')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It works for these two examples, so we have a proof of concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['New York State Route 373',\n",
       " 'St. Lawrence County, New York',\n",
       " 'Hopkinton, New York',\n",
       " 'Adirondack Park',\n",
       " 'Ausable River (New York)',\n",
       " 'Keeseville, New York',\n",
       " 'Chesterfield, New York',\n",
       " 'Port Kent, New York',\n",
       " 'Essex County, New York',\n",
       " 'Albany, New York',\n",
       " 'Plattsburgh, New York',\n",
       " 'Lake Champlain Transportation Company',\n",
       " 'Lake Champlain',\n",
       " 'New York (state)',\n",
       " 'New York State Legislature',\n",
       " '52nd New York State Legislature',\n",
       " 'United States',\n",
       " 'Amtrak',\n",
       " 'Canadian Pacific Railway',\n",
       " 'Vermont',\n",
       " 'Burlington, Vermont',\n",
       " 'Portland, Maine',\n",
       " 'Theodore Roosevelt International Highway',\n",
       " 'Auto trail',\n",
       " 'Baltimore, Maryland',\n",
       " 'Portland, Oregon',\n",
       " 'American Antiquarian Society',\n",
       " 'Henry Louis Gates']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search('New York State Route 373', 'Henry Louis Gates')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Only one page links to Henry Louis Gates, which is American Antiquarian Society). Unfortunately American Antiquarian Society is semantically very far from Henry Louis Gates, so it takes a long time for the algorithm to click on American Antiquarian Society, even though it's actually linked to by the starting page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "To continue this project, we want to scrape a larger subset of Wikipedia pages and test the graph search and greedy algorithms on the larger graph. This week showed us that this is actually a significant challenge, so we'll need to dedicate more time to the problem of scraping beyond 2 degrees of separation. We want to compare the performance of the two algorithms identical start-end points and see if the greedy algorithm can match the performance of the graph search algorithm. It will be interesting to compare their runtimes as well as their minimum-length paths.\n",
    "\n",
    "We're pretty satisfied with our progress this week, and we think that we're on track to make a cool final project.\n",
    "\n",
    "[Draft presentation](https://docs.google.com/presentation/d/1GxHRYCPbECe-QAT_uPcoYR31yHpl1f1XA6l_k-nwTvw/edit?usp=sharing)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c816ed2e4fb958a73b47f4f7bcb3cccc66deb384cf692b066fe3b8c8b16294f"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
