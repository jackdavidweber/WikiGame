{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Wikipedia Game\n",
    "The goal of this project is to build an algorithm that is able to perform better than a random player at the wikipedia game! The wikipedia game is [INSERT INFO ABOUT WIKIPEDIA GAME]. \n",
    "\n",
    "We've broken the project into three main technical challenges\n",
    "* Scrape wikipedia (or at least a subset) in order to build a data structure with information about what links exist between pages.\n",
    "* Build graph algorithms and visualizations for finding the shortest distance between two pages.\n",
    "* Create an algorithm that does not rely on the graph but can still navigate from one page to another in a short distance. Ideally this distance would be equal to the shortest possible distance, but it would be cool if it at least performed better than an algorithm that chooses links randomly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Scraping Wikipedia\n",
    "The easiest way to do this would probably have been to rely on the wikipedia data dumps that are released twice monthly. However, we wanted to play with beautiful soup and requests, so we decided to implement the scraping manually. This is much more time intensive (computationally) which means that we are not able to build a graph of all of wikipedia, but we also found it to be way cooler!\n",
    "\n",
    "Before we could actually scrape wikipedia, we had to decide what data we wanted to store and how we wanted to store it. After looking through a bunch of articles, we found that all wikipedia pages have the same link structure:\n",
    "```\n",
    "https://en.wikipedia.org/wiki/[articleTitle]\n",
    "```\n",
    "So the article on Star Wars has the link https://en.wikipedia.org/wiki/Star_Wars and the article entitled \"The Hitchhiker's Guide to the Galaxy\" has the link https://en.wikipedia.org/wiki/The_Hitchhiker%27s_Guide_to_the_Galaxy (notice how the apostraphe is handled!).\n",
    "\n",
    "This meant that as long as we stored the article title, we could always recreate the link for the article.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The title of the requested page is <title>The Hitchhiker's Guide to the Galaxy - Wikipedia</title>\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup     \n",
    "\n",
    "PARSER = \"lxml\"\n",
    "\n",
    "title = r\"The_Hitchhiker%27s_Guide_to_the_Galaxy\"\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/\" + title\n",
    "response = requests.get(wiki_url)\n",
    "soup = BeautifulSoup(response.text,PARSER)\n",
    "\n",
    "print(f\"The title of the requested page is {soup.title}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple way of representing graphs is with dictionaries. Every key in the dictionary represents a node of the graph. The value associated with the key is a list of all of the nodes that the the key is connected to. This is how we decided to represent the wikipedia graph. Let's take a look at what the Star Wars key would look like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): Star_Wars\n",
      "\n",
      "Star_Wars links to 1259 other wikipedia articles\n",
      "For example, within the Star_Wars article, there are links to the following wikipedia pages:\n",
      "\tThe_Empire_Strikes_Back\n",
      "\tCasey_Hudson\n",
      "\tRail_shooter\n",
      "\tOuija\n",
      "\tA-wing\n",
      "\tStretch_Armstrong\n",
      "\tAnthony_Breznican\n",
      "\tHarrison_Ford\n",
      "\tThe_Rising_Force\n",
      "\tMr._Potato_Head\n"
     ]
    }
   ],
   "source": [
    "from scraping import get_wiki_graph_one_step, get_wiki_graph\n",
    "\n",
    "title= \"Star_Wars\"\n",
    "d = get_wiki_graph_one_step([title])\n",
    "\n",
    "print()\n",
    "print(f\"{title} links to {len(d[title])} other wikipedia articles\")\n",
    "print(f\"For example, within the {title} article, there are links to the following wikipedia pages:\")\n",
    "for i in range(0,min(10, len(d[title]))):\n",
    "    print(f\"\\t{d[title][i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above, we just requested the star wars wikipedia page. But in order to build a bigger graph, we could then repeat the same process for all of the pages referenced in the star wars page. We could continue to repeat this process for as many steps as we'd like.\n",
    "\n",
    "Let's try this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): New_York_State_Route_373\n",
      "(0/59): The_New_York_Times\n",
      "(1/59): Numbered_highways_in_New_York\n",
      "(2/59): Lake_Champlain\n",
      "(3/59): United_States\n",
      "(4/59): 52nd_New_York_State_Legislature\n",
      "(5/59): Vermont\n",
      "(6/59): Parkways_in_New_York\n",
      "(7/59): ISBN_(identifier)\n",
      "(8/59): Main_Page\n",
      "(9/59): Theodore_Roosevelt_International_Highway\n",
      "(10/59): Ausable_Chasm\n",
      "(11/59): American_Antiquarian_Society\n",
      "(12/59): Canadian_Pacific_Railway\n",
      "(13/59): New_York_State_Legislature\n",
      "(14/59): Keeseville,_New_York\n",
      "(15/59): Hamlet_(New_York)\n",
      "(16/59): St._Lawrence_County,_New_York\n",
      "(17/59): List_of_reference_routes_in_New_York\n",
      "(18/59): Burlington,_Vermont\n",
      "(19/59): New_York_State_Route_374\n",
      "(20/59): Plattsburgh,_New_York\n",
      "(21/59): General_Drafting\n",
      "(22/59): Ausable_Chasm,_New_York\n",
      "(23/59): U.S._Route_9_in_New_York\n",
      "(24/59): County_Route_17_(Essex_County,_New_York)\n",
      "(25/59): Standard_Oil_Company_of_New_York\n",
      "(26/59): List_of_Interstate_Highways_in_New_York\n",
      "(27/59): Burlington%E2%80%93Port_Kent_Ferry\n",
      "(28/59): Portland,_Oregon\n",
      "(29/59): Portland,_Maine\n",
      "(30/59): Port_Kent_(Amtrak_station)\n",
      "(31/59): Lake_Champlain_Transportation_Company#Burlington-Port_Kent\n",
      "(32/59): Google_Maps\n",
      "(33/59): List_of_U.S._Routes_in_New_York\n",
      "(34/59): List_of_state_routes_in_New_York\n",
      "(35/59): Adirondack_Park\n",
      "(36/59): Lake_Champlain_Transportation_Company\n",
      "(37/59): History_of_the_iron_and_steel_industry_in_the_United_States#Early_republic\n",
      "(38/59): Google\n",
      "(39/59): New_York_State_Route_372\n",
      "(40/59): Chesterfield,_New_York\n",
      "(41/59): Auto_trail\n",
      "(42/59): Toll_road\n",
      "(43/59): New_York_(state)\n",
      "(44/59): 1930_state_highway_renumbering_(New_York)\n",
      "(45/59): Interstate_87_(New_York)\n",
      "(46/59): Amtrak\n",
      "(47/59): Port_Kent_and_Hopkinton_Turnpike\n",
      "(48/59): Essex_County,_New_York\n",
      "(49/59): Port_Kent,_New_York\n",
      "(50/59): County_Route_71_(Essex_County,_New_York)\n",
      "(51/59): Hopkinton,_New_York\n",
      "(52/59): Albany,_New_York\n",
      "(53/59): Toll_gate\n",
      "(54/59): Reference_route_(New_York)\n",
      "(55/59): Ausable_River_(New_York)\n",
      "(56/59): New_York_Constitution#Constitutional_Convention_of_1894\n",
      "(57/59): Baltimore,_Maryland\n",
      "(58/59): New_York_State_Department_of_Transportation\n"
     ]
    }
   ],
   "source": [
    "title = \"New_York_State_Route_373\"\n",
    "out = get_wiki_graph(starting_refs=[title], num_steps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New_York_State_Route_373 has 60 links\n",
      "One such link is The_New_York_Times which itself has 927 links all of which link out to other articles\n"
     ]
    }
   ],
   "source": [
    "d = out[0]\n",
    "print(f\"{title} has {len(d[title])} links\")\n",
    "\n",
    "title_1 = d[title][0]\n",
    "print(f\"One such link is {title_1} which itself has {len(d[title_1])} links all of which link out to other articles\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we did this for two steps. In the first step we found all of the links in `New_York_State_Route_373`. In the second step we found all of the links within all of these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If we were to do the third step, we would have to find all of the links within 19530 wikipedia articles\n",
      "Given that this takes about 0.5 seconds per article, just three steps would take 9765.0 seconds or 2.7125 hours\n"
     ]
    }
   ],
   "source": [
    "print(f\"If we were to do the third step, we would have to find all of the links within {len(out[2])} wikipedia articles\")\n",
    "\n",
    "print(f\"Given that this takes about 0.5 seconds per article, just three steps would take {len(out[2])*0.5} seconds or {len(out[2])*0.5/3600} hours\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With every additional step we take beyond the initial article, the time to build the graph increases exponentially. Since we didn't want our personal computers to be occupied for multiple days, we did this on the Pomona servers using the `scraping.py` file we wrote. The final product was a .json file with [DESCRIBE NUMBER OF ENTRIES]. In the next section, we will be working with this json to build cool graph visualizations and algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph Visualizations and Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup     \n",
    "import requests\n",
    "\n",
    "PARSER = \"lxml\"            # to use lxml (the most common), you'll need to install with .../pip install lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "star_wars_url = \"https://en.wikipedia.org/wiki/Star_Wars\"\n",
    "response = requests.get(star_wars_url)\n",
    "data_from_url = response.text\n",
    "soup = BeautifulSoup(data_from_url,PARSER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue = [\"Star Wars\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_graph_one_step(l):\n",
    "    \"\"\"\n",
    "    Takes a list of wikipedia articles with no repeats. \n",
    "    Returns A dictionary where keys are items in original list. \n",
    "    Values are lists of references to other wikipedia articles\n",
    "    \"\"\"\n",
    "    d = {}\n",
    "    # s = set()\n",
    "\n",
    "    for i in range(len(l)):\n",
    "        title = l[i]\n",
    "        wiki_url = \"https://en.wikipedia.org/wiki/\" + title\n",
    "\n",
    "        # Request Wikipedia and Parse\n",
    "        response = requests.get(wiki_url)\n",
    "        data_from_url = response.text\n",
    "        soup = BeautifulSoup(data_from_url,PARSER)\n",
    "\n",
    "        print(f\"({i}/{len(l)}): {title}\")\n",
    "\n",
    "        # Capture all referenced articles within article\n",
    "        link_set = set()  # Use a set to ensure no repeats\n",
    "        for link in soup.find_all('a'):\n",
    "            s = link.get('href')\n",
    "            if (s and s[:6] == \"/wiki/\"):\n",
    "                ref = s[6:]\n",
    "\n",
    "                # Make sure that title does not include \":\" (means it is not normal wikipedia page)\n",
    "                if (ref.find(\":\") == -1):\n",
    "                    link_set.add(ref)\n",
    "        d[title] = list(link_set)\n",
    "\n",
    "    return d\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_of_lists_to_set(lol):\n",
    "    s = set()\n",
    "    for l in lol:\n",
    "        for item in l:\n",
    "            s.add(item)\n",
    "\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dict(d, filename):\n",
    "    # create json object from dictionary\n",
    "    json_file = json.dumps(d)\n",
    "\n",
    "    # open file for writing, \"w\" \n",
    "    f = open(filename,\"w\")\n",
    "\n",
    "    # write json object to file\n",
    "    f.write(json_file)\n",
    "\n",
    "    # close file\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wiki_graph(final_d = {}, starting_refs=[], num_steps=1, filename=\"wiki_graph\"):\n",
    "    if starting_refs:\n",
    "        ref_list = starting_refs\n",
    "    else:\n",
    "        ref_list = [\"New_York_State_Route_373\"]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        step_d = get_wiki_graph_one_step(ref_list)\n",
    "        final_d.update(step_d)\n",
    "\n",
    "        # Get all of the references that haven't been added to graph\n",
    "        step_refs = list_of_lists_to_set(list(step_d.values()))\n",
    "        existing_refs = set(final_d.keys())\n",
    "        unseen_refs = step_refs.difference(existing_refs)\n",
    "\n",
    "        ref_list = list(unseen_refs)\n",
    "        \n",
    "        save_dict(final_d, f\"{filename}_{i}.json\")\n",
    "\n",
    "    return (final_d, step_refs, ref_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0/1): New_York_State_Route_373\n",
      "(0/59): Auto_trail\n",
      "(1/59): Parkways_in_New_York\n",
      "(2/59): Chesterfield,_New_York\n",
      "(3/59): Port_Kent_and_Hopkinton_Turnpike\n",
      "(4/59): Burlington,_Vermont\n",
      "(5/59): Plattsburgh,_New_York\n",
      "(6/59): ISBN_(identifier)\n",
      "(7/59): County_Route_17_(Essex_County,_New_York)\n",
      "(8/59): 52nd_New_York_State_Legislature\n",
      "(9/59): Interstate_87_(New_York)\n",
      "(10/59): Lake_Champlain\n",
      "(11/59): New_York_State_Legislature\n",
      "(12/59): Theodore_Roosevelt_International_Highway\n",
      "(13/59): Baltimore,_Maryland\n",
      "(14/59): Hopkinton,_New_York\n",
      "(15/59): New_York_State_Route_372\n",
      "(16/59): Lake_Champlain_Transportation_Company#Burlington-Port_Kent\n",
      "(17/59): General_Drafting\n",
      "(18/59): Lake_Champlain_Transportation_Company\n",
      "(19/59): Port_Kent_(Amtrak_station)\n",
      "(20/59): Albany,_New_York\n",
      "(21/59): New_York_State_Route_374\n",
      "(22/59): Ausable_Chasm,_New_York\n",
      "(23/59): Amtrak\n",
      "(24/59): Toll_gate\n",
      "(25/59): Hamlet_(New_York)\n",
      "(26/59): Google_Maps\n",
      "(27/59): List_of_U.S._Routes_in_New_York\n",
      "(28/59): Ausable_Chasm\n",
      "(29/59): Toll_road\n",
      "(30/59): List_of_state_routes_in_New_York\n",
      "(31/59): Portland,_Maine\n",
      "(32/59): 1930_state_highway_renumbering_(New_York)\n",
      "(33/59): United_States\n",
      "(34/59): Standard_Oil_Company_of_New_York\n",
      "(35/59): Main_Page\n",
      "(36/59): St._Lawrence_County,_New_York\n",
      "(37/59): Canadian_Pacific_Railway\n",
      "(38/59): Vermont\n",
      "(39/59): Port_Kent,_New_York\n",
      "(40/59): Adirondack_Park\n",
      "(41/59): U.S._Route_9_in_New_York\n",
      "(42/59): Burlington%E2%80%93Port_Kent_Ferry\n",
      "(43/59): New_York_(state)\n",
      "(44/59): Portland,_Oregon\n",
      "(45/59): List_of_Interstate_Highways_in_New_York\n",
      "(46/59): Reference_route_(New_York)\n",
      "(47/59): New_York_State_Department_of_Transportation\n",
      "(48/59): The_New_York_Times\n",
      "(49/59): Ausable_River_(New_York)\n",
      "(50/59): New_York_Constitution#Constitutional_Convention_of_1894\n",
      "(51/59): County_Route_71_(Essex_County,_New_York)\n",
      "(52/59): History_of_the_iron_and_steel_industry_in_the_United_States#Early_republic\n",
      "(53/59): Keeseville,_New_York\n",
      "(54/59): Google\n",
      "(55/59): Essex_County,_New_York\n",
      "(56/59): List_of_reference_routes_in_New_York\n",
      "(57/59): American_Antiquarian_Society\n",
      "(58/59): Numbered_highways_in_New_York\n"
     ]
    }
   ],
   "source": [
    "final_d, step_refs, ref_list = get_wiki_graph(starting_refs=[\"New_York_State_Route_373\"], num_steps=4, filename=\"wiki_graph_NYSR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5c816ed2e4fb958a73b47f4f7bcb3cccc66deb384cf692b066fe3b8c8b16294f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('pyDs')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
